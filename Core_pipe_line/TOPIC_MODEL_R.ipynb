{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the data from corpus and building the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# panda and sqlite import\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# data base path\n",
    "db_path = 'C:/Users/8897p/OneDrive/Desktop/NLP/Project/AXR/processed_articles.db'\n",
    "\n",
    "# open the connection to the data base\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# create the pandas data frame with full table data\n",
    "query = \"SELECT * FROM articles\"\n",
    "df_corpus = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Close the connection after loading the data\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom method to extract the word frequency vectors from highlights text.\n",
    "\n",
    "Tunning done on how often the word occurs and how least it occurs, eliminating the outliers from the topic modeling consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# custom function to process the text and vectorize \n",
    "def preprocess_text(text_corpus):\n",
    "    # Addition words to ignore\n",
    "    custom_stop_words = ['day', 'say', 'says', 'time', 'got', 'new', 'week','said']  # Replace with the words you want to remove\n",
    "\n",
    "    # Combining them with default English stop words\n",
    "    combined_stop_words = set(custom_stop_words).union(set(CountVectorizer(stop_words='english').get_stop_words()))\n",
    "\n",
    "    # Create the vercorizer\n",
    "    vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words=list(combined_stop_words)) # parameter tunning of max and minimum frequencies\n",
    "    dt_matrix = vectorizer.fit_transform(text_corpus)\n",
    "    return dt_matrix, vectorizer\n",
    "\n",
    "# generating the document frequncy model and getting the learned vectorizer on vocabulary\n",
    "dt_matrix, vectorizer = preprocess_text(df_corpus['highlights'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the word vectors to implement the LDA algorithm to find the best words to model the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Creating the LDA object\n",
    "num_topics = 4\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=123) # number of topics and random state set for easy tunning, same results on different parameters\n",
    "\n",
    "# Fit the LDA model to the document-term matrix and retrieve the topic distribution\n",
    "lda.fit(dt_matrix)\n",
    "lda_topic_dist = lda.transform(dt_matrix)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating throught the topics and displaying them. Sorted in as low to high value. Top 5 are the last 5 topic word distribution rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:  ['security', 'suicide', 'house', 'iraq', 'people', 'told', 'attack', 'syria', 'mama', 'president']\n",
      "Topic 2:  ['hospital', 'america', 'killed', 'world', 'official', 'died', 'children', 'united', 'police', 'people']\n",
      "Topic 3:  ['women', 'home', 'inland', 'china', 'long', 'baby', 'just', 'australia', 'people', 'year']\n",
      "Topic 4:  ['rock', 'world', 'year', 'reported', 'people', 'games', 'shake', 'seen', 'young', 'white']\n"
     ]
    }
   ],
   "source": [
    "# Custom function to display topics and their top words\n",
    "def display_topics(model, feature_names, num_top_words=10):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx + 1}: \", [feature_names[i] for i in topic.argsort()[-num_top_words:]])\n",
    "\n",
    "# Display topics\n",
    "display_topics(lda, vectorizer.get_feature_names_out())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assigning topic numbers and custom labels for furthur processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>highlights</th>\n",
       "      <th>assigned_topic</th>\n",
       "      <th>topic_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>japan's chief cabinet secretary Yoshihide suga...</td>\n",
       "      <td>3</td>\n",
       "      <td>Popular Culture and Key Events</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>syria official mama climbed to the top of the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Middle East Affairs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The employee in agency canvas City office is a...</td>\n",
       "      <td>2</td>\n",
       "      <td>Domestic and Social Issues in China and Australia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NEW A canada doctor says she was part of a tea...</td>\n",
       "      <td>3</td>\n",
       "      <td>Popular Culture and Key Events</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NEW 4 groups announce legal challenge in Phoen...</td>\n",
       "      <td>3</td>\n",
       "      <td>Popular Culture and Key Events</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          highlights  assigned_topic  \\\n",
       "0  japan's chief cabinet secretary Yoshihide suga...               3   \n",
       "1  syria official mama climbed to the top of the ...               0   \n",
       "2  The employee in agency canvas City office is a...               2   \n",
       "3  NEW A canada doctor says she was part of a tea...               3   \n",
       "4  NEW 4 groups announce legal challenge in Phoen...               3   \n",
       "\n",
       "                                         topic_label  \n",
       "0                     Popular Culture and Key Events  \n",
       "1                                Middle East Affairs  \n",
       "2  Domestic and Social Issues in China and Australia  \n",
       "3                     Popular Culture and Key Events  \n",
       "4                     Popular Culture and Key Events  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assigning a topic out of the 4 modeled\n",
    "df_corpus['assigned_topic'] = lda_topic_dist.argmax(axis=1)\n",
    "\n",
    "# Custom labels for the topic document probabilities\n",
    "topic_labels = {\n",
    "    0: \"Middle East Affairs\",\n",
    "    1: \"Safty of children\",\n",
    "    2: \"Domestic and Social Issues in China and Australia\",\n",
    "    3: \"Popular Culture and Key Events\",\n",
    "}\n",
    "\n",
    "# Map for the numeric to text form of topics\n",
    "df_corpus['topic_label'] = df_corpus['assigned_topic'].map(topic_labels)\n",
    "\n",
    "# Sample head display\n",
    "df_corpus[['highlights', 'assigned_topic', 'topic_label']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          highlights  assigned_topic  \\\n",
      "0  japan's chief cabinet secretary Yoshihide suga...               3   \n",
      "1  syria official mama climbed to the top of the ...               0   \n",
      "2  The employee in agency canvas City office is a...               2   \n",
      "3  NEW A canada doctor says she was part of a tea...               3   \n",
      "4  NEW 4 groups announce legal challenge in Phoen...               3   \n",
      "\n",
      "                                         topic_label  \n",
      "0                     Popular Culture and Key Events  \n",
      "1                                Middle East Affairs  \n",
      "2  Domestic and Social Issues in China and Australia  \n",
      "3                     Popular Culture and Key Events  \n",
      "4                     Popular Culture and Key Events  \n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# create the new Database with topics\n",
    "conn = sqlite3.connect('Modeler_output_topics.db')\n",
    "\n",
    "# Data frame to data base convertion\n",
    "df_corpus.to_sql('topic_data', conn, if_exists='replace', index=False)\n",
    "\n",
    "# Validation of the data inserted\n",
    "query = \"SELECT highlights, assigned_topic, topic_label FROM topic_data LIMIT 5;\"\n",
    "df_verification = pd.read_sql(query, conn)\n",
    "print(df_verification)\n",
    "\n",
    "# Close connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity and Coherence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 1544.3355650432766\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate log likelihood\n",
    "log_likelihood = lda.score(dt_matrix)\n",
    "\n",
    "# Calculate perplexity\n",
    "perplexity = np.exp(-log_likelihood / dt_matrix.sum())\n",
    "print(f'Perplexity: {perplexity}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code to open the db, preprocess the highlights column. Creating an new column with preprocessed data for gensim's LDA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\8897p\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\8897p\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import download\n",
    "\n",
    "# Downloading the wordnet dictionary and stopwords corpus from nltk\n",
    "download('stopwords')\n",
    "download('wordnet')\n",
    "\n",
    "# open the connection to the data base\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Loading the data base of preprocessed text\n",
    "query = \"SELECT * FROM articles\"\n",
    "df_corpus = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Close the connection after loading the data\n",
    "conn.close()\n",
    "\n",
    "# create instances for stop_words and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Custom method to process the text for gensim LDA model\n",
    "def preprocess(text):\n",
    "    \n",
    "    tokens = simple_preprocess(text) # tokenzie\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words] # lemmatize and form a list of tokens\n",
    "    return tokens\n",
    "\n",
    "# Using the custom function on the data frame with clean text\n",
    "df_corpus['processed_text'] = df_corpus['highlights'].apply(preprocess)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the Dictonary as processed text field is passed to it. Corpus is the creation of the bag of words that will help the model to process the topics from the text provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dictionary for gensim and the bag of words data corpus\n",
    "dictionary_gensim = Dictionary(df_corpus['processed_text'])\n",
    "corpus = [dictionary_gensim.doc2bow(text) for text in df_corpus['processed_text']]\n",
    "\n",
    "\n",
    "# Training the lda model on the corpus and the created word_id dictionary from bag of words\n",
    "lda_model_gensim = LdaModel(corpus=corpus, id2word=dictionary_gensim, num_topics=11, random_state=0, passes=12, alpha=0.5, eta=0.01)# hyper tunning done on number of topics, number of passes, alpha and eta values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation on the Gensim's LDA model. Calculating the negative perplexity and the coherence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.4369186770548062\n",
      "Perplexity: -9.58770659710414\n"
     ]
    }
   ],
   "source": [
    "# import for the coherence evaluation\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Calculation of the coherence scores on hte gensim_lda model\n",
    "coherence_model_lda = CoherenceModel(model=lda_model_gensim, texts=df_corpus['processed_text'], dictionary=dictionary_gensim, coherence='c_v')\n",
    "coherence_score = coherence_model_lda.get_coherence()\n",
    "print(f'Coherence Score: {coherence_score}')\n",
    "\n",
    "# Calculating the Perplexity of the lda model gensim\n",
    "perplexity = lda_model_gensim.log_perplexity(corpus)\n",
    "print(f'Perplexity: {perplexity}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the Gensim, model topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 7:  say, mama, u, new, president, syria, iraq, si, drug, attack\n",
      "Topic 10:  john, inland, world, plan, month, hit, australia, cup, first, player\n",
      "Topic 0:  say, police, christian, monk, history, man, family, said, turkey, situation\n",
      "Topic 2:  say, told, woman, could, young, got, school, group, hospital, presidential\n",
      "Topic 9:  say, one, president, year, two, doctor, mama, video, city, last\n",
      "Topic 4:  claim, say, also, one, since, case, could, police, people, life\n",
      "Topic 3:  new, people, say, attack, killed, sanction, go, white, bank, u\n",
      "Topic 8:  say, like, new, u, first, xu, china, people, bone, euro\n",
      "Topic 5:  say, child, united, new, night, time, bone, service, goal, draw\n",
      "Topic 1:  new, year, america, shake, besiktas, say, law, first, iran, gunner\n"
     ]
    }
   ],
   "source": [
    "# Print topics in a cleaner format\n",
    "for idex_topic, topic in lda_model_gensim.show_topics(formatted=False, num_words=10):\n",
    "    print(f\"Topic {idex_topic}: \", \", \".join([word for word, prob in topic]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          highlights  assigned_topic  \\\n",
      "0  japan's chief cabinet secretary Yoshihide suga...               7   \n",
      "1  syria official mama climbed to the top of the ...               7   \n",
      "2  The employee in agency canvas City office is a...               8   \n",
      "3  NEW A canada doctor says she was part of a tea...               5   \n",
      "4  NEW 4 groups announce legal challenge in Phoen...               1   \n",
      "\n",
      "                            topic_label  \n",
      "0     Law Enforcement and Public Safety  \n",
      "1     Law Enforcement and Public Safety  \n",
      "2              China and Global Affairs  \n",
      "3                     Sports and Events  \n",
      "4  International Relations and Politics  \n",
      "                                          highlights  assigned_topic  \\\n",
      "0  japan's chief cabinet secretary Yoshihide suga...               7   \n",
      "1  syria official mama climbed to the top of the ...               7   \n",
      "2  The employee in agency canvas City office is a...               8   \n",
      "3  NEW A canada doctor says she was part of a tea...               5   \n",
      "4  NEW 4 groups announce legal challenge in Phoen...               1   \n",
      "\n",
      "                            topic_label  \n",
      "0     Law Enforcement and Public Safety  \n",
      "1     Law Enforcement and Public Safety  \n",
      "2              China and Global Affairs  \n",
      "3                     Sports and Events  \n",
      "4  International Relations and Politics  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# Retrieve the topic distribution for each document\n",
    "doc_topics_gensim = [lda_model_gensim.get_document_topics(bow) for bow in corpus]\n",
    "\n",
    "# Extract the max probable topics and assign them to the documents\n",
    "df_corpus['assigned_topic'] = [max(doc, key=lambda x: x[1])[0] for doc in doc_topics_gensim]\n",
    "\n",
    "# These are the custom labels for the topics - will be assigned across the 200 documents\n",
    "topic_labels = {\n",
    "    0: \"Religion and Cultural Affairs\",\n",
    "    1: \"International Relations and Politics\",\n",
    "    2: \"Social Issues and Health\",\n",
    "    3: \"Conflict and Security\",\n",
    "    4: \"Crime and Legal Proceedings\",\n",
    "    5: \"Sports and Events\",\n",
    "    6: \"Political Leadership and Governance\",\n",
    "    7: \"Law Enforcement and Public Safety\",\n",
    "    8: \"China and Global Affairs\",\n",
    "    9: \"Politics and Society\",\n",
    "    10: \"Sports and International Competitions\"\n",
    "}\n",
    "\n",
    "# Numeric label map to the custom labels created and assigned above\n",
    "df_corpus['topic_label'] = df_corpus['assigned_topic'].map(topic_labels)\n",
    "\n",
    "# Show few topics as validation for the assignment\n",
    "print(df_corpus[['highlights', 'assigned_topic', 'topic_label']].head())\n",
    "\n",
    "# Removign the process_text columns from the data frame\n",
    "df_corpus_filtered = df_corpus.drop(columns=['processed_text'], errors='ignore')\n",
    "\n",
    "# Creating the data base and the table to store the processed data frame\n",
    "conn = sqlite3.connect('Modeler_output_topics_gensim.db')\n",
    "df_corpus_filtered.to_sql('topic_data_gensim', conn, if_exists='replace', index=False)\n",
    "\n",
    "# Data base verification on the data insertion\n",
    "query = \"SELECT highlights, assigned_topic, topic_label FROM topic_data_gensim LIMIT 5;\"\n",
    "df_verification = pd.read_sql(query, conn)\n",
    "print(df_verification)\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique topics in the database:\n",
      "    assigned_topic\n",
      "0                7\n",
      "1                8\n",
      "2                5\n",
      "3                1\n",
      "4                3\n",
      "5               10\n",
      "6                6\n",
      "7                4\n",
      "8                2\n",
      "9                0\n",
      "10               9\n",
      "\n",
      "Unique topic labels in the database:\n",
      "                              topic_label\n",
      "0       Law Enforcement and Public Safety\n",
      "1                China and Global Affairs\n",
      "2                       Sports and Events\n",
      "3    International Relations and Politics\n",
      "4                   Conflict and Security\n",
      "5   Sports and International Competitions\n",
      "6     Political Leadership and Governance\n",
      "7             Crime and Legal Proceedings\n",
      "8                Social Issues and Health\n",
      "9           Religion and Cultural Affairs\n",
      "10                   Politics and Society\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('Modeler_output_topics_gensim.db')\n",
    "\n",
    "# Query to get the unique topics in the 'assigned_topic' column\n",
    "query = \"SELECT DISTINCT assigned_topic FROM topic_data_gensim;\"\n",
    "unique_topics = pd.read_sql(query, conn)\n",
    "\n",
    "# Display the unique topics to verify all 11 are present\n",
    "print(\"Unique topics in the database:\")\n",
    "print(unique_topics)\n",
    "\n",
    "# Optionally, you can also check topic labels\n",
    "query_labels = \"SELECT DISTINCT topic_label FROM topic_data_gensim;\"\n",
    "unique_labels = pd.read_sql(query_labels, conn)\n",
    "\n",
    "print(\"\\nUnique topic labels in the database:\")\n",
    "print(unique_labels)\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
